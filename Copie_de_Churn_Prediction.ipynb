{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/MLW_novembre/blob/main/Copie_de_Churn_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6febe8cd",
      "metadata": {
        "id": "6febe8cd"
      },
      "source": [
        "# TP : Prédiction du Churn avec du machine learning\n",
        "\n",
        "## Objectif du TP\n",
        "\n",
        "L'objectif de ce TP est de créer une API de prédiction du churn (attrition) qui utilise des algorithmes de machine learning. L'API permettra de recevoir des données clients sous forme de requêtes HTTP et, grâce au modèle de machine learning, retourne une prédiction, si un client est susceptible de se désabonner ou non. L'API sera développée en utilisant le framework `Flask` et permettra d'exposer facilement le modèle via un endpoint accessible localement dans un premier temps.Dans un second temps, l'APi sera empaquetée dans un environnement `Docker` et déployée sur un cloud.\n",
        "\n",
        "Ce TP couvrira plusieurs étapes clés : la préparation et l'entraînement du modèle de machine learning, la création et la sérialisation du modèle, la conception de l'API Flask, ainsi que son déploiement dans un conteneur Docker. Les participants auront également l'occasion de tester l'API en envoyant des requêtes avec des outils tels que Postman ou curl, tout en explorant des concepts liés à la conteneurisation avec Docker, facilitant ainsi l'intégration continue et la scalabilité des solutions d'IA\n",
        "\n",
        "\n",
        "## Partie I\n",
        "\n",
        "### Étape 1 : Préparation des données et entraînement du modèle de prédiction (ML)\n",
        "\n",
        "1. **Analyse exploratoire des données (EDA)** : Explorer le dataset fourni, comprendre les caractéristiques des clients et des churns. Utiliser des bibliothèques comme `pandas` et `matplotlib` pour visualiser les données et identifier les tendances.\n",
        "   \n",
        "\n",
        "2. **Prétraitement des données** :\n",
        "   Divisez les données en un ensemble d'entraînement et de test.\n",
        "\n",
        "3. **Entraînement d'un modèle de machine learning** :\n",
        "   Utilisez `statsmodels` pour entraîner un modèle de régression logistique. Séléectionner le modèle le plus    pertinent.\n",
        "\n",
        "4. **Sérialisation du modèle** :\n",
        "   Une fois le modèle entraîné, sauvegardez-le sous format`pickle` à l'aide de `joblib` pour pouvoir le charger plus tard dans l'API Flask.\n",
        "\n",
        "\n",
        "### Étape 2 : Création de l'API Flask\n",
        "\n",
        "1. **Création de l'API** :\n",
        "   A l'aide de **Flask**, exposer le modèle en déployant, en local, une API qui recevra les données des clients en entrée et renverra une prédiction (churn ou non churn).\n",
        "\n",
        "\n",
        "2. **Endpoints de l'API** :\n",
        "   L'API doit avoir un endpoint `POST /predict` qui acceptera les caractéristiques d'un client sous forme de **JSON** et retournera une prédiction (churn ou non churn).\n",
        "\n",
        "\n",
        "3. **Chargement du modèle dans l'API** :\n",
        "   Le modèle pré-entraîné sera chargé avec `pickle` et utilisé pour générer des prédictions lors des appels à l'API.\n",
        "\n",
        "\n",
        "### Étape 3 : Test de l'API Flask\n",
        "\n",
        "1. **Test de l'API** :\n",
        "   A l'aide de **request**, tester l'API en local envoyant des requêtes via le endpoint `POST /predict`.\n",
        "   \n",
        "\n",
        "\n",
        "2. **Optionnel** :\n",
        "   Créer une interface `HTM` pour interagit avec l'API d'une manière plus simple et plus intuitive.\n",
        "---\n",
        "\n",
        "## Partie II\n",
        "\n",
        "### Étape 1 : Conteneurisation avec Docker\n",
        "\n",
        "1. **Création du fichier Dockerfile** :\n",
        "   Créez un fichier `Dockerfile` pour empaqueter l'application Flask, le modèle de machine learning et toutes les dépendances nécessaires (par exemple, `Flask`, `scikit-learn`, `pandas`).\n",
        "\n",
        "```\n",
        "   churn-app/\n",
        "   │\n",
        "   ├── app.py                 # Le script principal de l'application Flask\n",
        "   ├── requirements.txt        # Les dépendances Python à installer\n",
        "   ├── Dockerfile              # Le fichier Docker pour conteneuriser l'application\n",
        "   ├── churn-model.pkl         # Le modèle de machine learning sérialisé\n",
        "   ├── data/\n",
        "   │   └── customer_churn.csv  # Le fichier CSV contenant les données des clients\n",
        "   └── templates/\n",
        "       └── index.html\n",
        "```\n",
        "   \n",
        "   \n",
        "2. **Exécution locale** :\n",
        "   Construisez l'image Docker et exécutez-la localement. Testez l'API en envoyant des requêtes POST avec des données client pour vérifier que les prédictions fonctionnent correctement.\n",
        "\n",
        "\n",
        "### Étape 2 : Déploiement sur le cloud\n",
        "\n",
        "1. **Déploiement de l'API** :\n",
        "   Déployer l'ensemble de l'application sur le cloud.\n",
        "\n",
        "\n",
        "2. **Test de l'API** :\n",
        "   Tester l'API via une interface `HTML`ou en envoyant des requêtes JSON avec les caractéristiques des clients pour obtenir des prédictions.\n",
        "\n",
        "\n",
        "3. **Documentation de l'API** :\n",
        "   Créer une documentation simple expliquant comment utiliser l'API, quels sont les endpoints disponibles et quelles sont les structures attendues des données d'entrée.\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}